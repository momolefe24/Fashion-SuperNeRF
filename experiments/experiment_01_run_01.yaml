---
# ------------------- Experiment Facts
experiment_facts:
 experiment_number: 1
 type: "Cat_NeRF" # [ESRGAN, Cat_NeRF, NeRF, Combined_NeRF_SR Viton, FashionNeRF]
 root_path: "./"
 run_number: 1
 experiment_name: "Concatenated Super Neural Radiance Field"
 experiment_description: "The idea is to forward propagate a NeRF and concatenate the predicted pixels and depth map into the super-resolution network and train it normally. The NeRF model is called inside the SRGAN method and trained adversarially"
 cuda: True


# ------------------- Checkpoint Facts
checkpoint_facts:
 load_checkpoint: False
 save_checkpoint: True
 root_path: "Checkpoints"
 NeRF:
  checkpoint_nerf: "NeRF.pth"
 ESRGAN:
  checkpoint_gen: "esrgan_gen.pth.tar"
  checkpoint_esrresnet: "esrresnet_gen.pth.tar"
  best_checkpoint_gen: "best_esrgan_gen.pth.tar"
  load_esrresnet: False
  load_esrgan: False
  load_p_best: False
  load_g_best: True
  esrgan: False
  vgg: False
  checkpoint_disc: "esrgan_disc.pth.tar"
 VITON:
  checkpoint_seg: "seg_final.pth"
  checkpoint_gmm: "gmm_final.pth"
  checkpoint_alias: "alias_final.pth"
  checkpoint_model: "viton.pth"



# ------------------- Result Facts
results_facts:
 experiment_number: 1
 run_number: 1
 root_path: "Results"
 basedir: "./logs"
 training_evaluation:
  training_loss: ""
  training_images: ""

# ------------------- Summary Facts
summary_facts:
  root_path: "Summary"
  ESRGAN:
   ground_truth: "ground_truth"
   sr: "sr"


# ------------------- Dataset Facts
dataset_facts:
 model: "eric"
 dataset_type: "blender"
 image:
  channels: 3
  lr_shape: (3,100,100)
  lr_shape_crop: (3,32,32)
  hr_shape: (3,1024,768)
  hr_shape_crop: (3, 256, 256)
 root_path: "Dataset"
 script_path: "dataset.py"
 transforms:
  mean:
   - 0.485
   - 0.456
   - 0.406
  std:
   - 0.229
   - 0.224
   - 0.225


# ------------------- Model Facts
model_facts:
 script_path:
  NeRF: "NeRF/model.py"
  SR: "ESRGAN/model.py"
  VITON: "VITON/model.py"
 in_channels: 3

# ------------------- Training Facts
training_facts:
 script_path: "train.py"
 batch_size: 2
 num_workers: 1
 NeRF:
  epochs: 5000
  betas: (0.5, 0.999)
  fc: 0.8
  dynamic_range: 255
  no_batching: True
  near: 2.0
  far: 6.0
  learning_rate: 2e-4
  num_layers: 8
  use_viewdirs: True
  white_bkgd: False
  lrate_decay: 500
  N_samples: 12
  N_importance: 128
  N_rand: 1024
  precrop_iters: 500
  precrop_frec: 0.5
  half_res: False
  pos_encode_dims: 16
 ESRGAN:
  epochs: 465
  upscaling_factor: 8
  high_res: 256
  p_epochs: 1162
  start_p_epoch: 0
  filters: 64
  num_res_blocks: 8
  upscale_factor: 10
  leaky_relu: 0.2
  features: [ 64, 64, 128, 128, 256,256, 512, 512, 1024, 1024 ]
  adaptive_average_pool: (12, 12)
  initialized_weight_scale: 0.1
  residual_in_residual_dense_block: 0.2
  lambda_gp: 10
  lambda_adv: 5e-3 # adversarial loss weight"
  content_weight: 1.0 # Content weight
  pixel_weight: 1e-2 # pixel-wise loss weight
  p_optimizer_lr: 0.0002 # Generator
  g_optimizer_lr: 0.0001 # Generator
  d_optimizer_lr: 0.0001 # Discriminator
  interpolation_mode: "nearest"
  learning_rate: 0.0002 # Adam
  betas: (0.9, 0.999)
  beta1: 0.9
  beta2: 0.999
  decay_epoch: 100
 VITON:
  epochs: 2000
  display_freq: 1
  semantic_nc: 13 # Number of parsing map classes
  init_type: "xavier"
  init_variance: 0.02 # Variance of the initialization distribution
  grid_size: 5 # For GMM
  alias_generator:
   norm_G: "spectralaliasinstance"
   ngf: 64 # Number of generator filters in the first conv layer
   num_upsampling_layers: "most"
  learning_rate: 1.0e-5
